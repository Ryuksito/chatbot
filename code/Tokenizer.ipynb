{"cells":[{"cell_type":"markdown","metadata":{"id":"310KFvsFq6Es"},"source":["# Test"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"K804fYx7q5Nk","executionInfo":{"status":"ok","timestamp":1725093004941,"user_tz":300,"elapsed":216,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"outputs":[],"source":["import re\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"zLOWZhOGWDe3","executionInfo":{"status":"ok","timestamp":1725093006976,"user_tz":300,"elapsed":262,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"outputs":[],"source":["corpus = [\n","    \"This is the Hugging Face Course.\",\n","    \"This chapter is about tokenization.\",\n","    \"This section shows several tokenizer algorithms.\",\n","    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6SZzboQrAQL"},"outputs":[],"source":["def pretokenize(text):\n","    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n","    return tokens\n","\n","def get_word_freqs(corpus):\n","    word_freqs = defaultdict(int)\n","    for text in corpus:\n","        for word in pretokenize(text):\n","            word_freqs[word] += 1\n","    return word_freqs\n","\n","def get_alphabet(word_freqs):\n","  alphabet = []\n","  for word in word_freqs.keys():\n","      if word[0] not in alphabet:\n","          alphabet.append(word[0])\n","      for letter in word[1:]:\n","          if f\"<pow>{letter}\" not in alphabet:\n","              alphabet.append(f\"<pow>{letter}\")\n","  alphabet.sort()\n","  return alphabet\n","\n","def get_vocab(alphabet):\n","  vocab = [\"<pad>\", \"<unk>\", \"<cls>\", \"<sep>\", \"<mask>\", \"<pow>\"] + alphabet.copy()\n","  return vocab\n","\n","def get_split_words(word_freqs):\n","  splits = {\n","    word: [c if i == 0 else f\"<pow>{c}\" for i, c in enumerate(word)] for word in word_freqs.keys()\n","  }\n","  return splits\n","\n","def compute_pair_scores(splits):\n","    letter_freqs = defaultdict(int)\n","    pair_freqs = defaultdict(int)\n","    for word, freq in word_freqs.items():\n","        split = splits[word]\n","        if len(split) == 1:\n","            letter_freqs[split[0]] += freq\n","            continue\n","        for i in range(len(split) - 1):\n","            pair = (split[i], split[i + 1])\n","            letter_freqs[split[i]] += freq\n","            pair_freqs[pair] += freq\n","        letter_freqs[split[-1]] += freq\n","\n","    scores = {\n","        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n","        for pair, freq in pair_freqs.items()\n","    }\n","    return scores\n","\n","def merge_pair(a, b, splits, word_freqs):\n","    for word in word_freqs:\n","        split = splits[word]\n","        if len(split) == 1:\n","            continue\n","        i = 0\n","        while i < len(split) - 1:\n","            if split[i] == a and split[i + 1] == b:\n","                merge = a + b[5:] if b.startswith(\"<pow>\") else a + b\n","                split = split[:i] + [merge] + split[i + 2 :]\n","            else:\n","                i += 1\n","        splits[word] = split\n","    return splits\n","\n","def combine_tokens(token_pair):\n","    tokens = []\n","\n","    for i, token in enumerate(token_pair):\n","\n","        if token.startswith(\"<pow>\"):\n","          tokens.append(token.replace(\"<pow>\", \"\", 1))\n","        else:\n","          tokens.append(token)\n","    combined_token = ''.join(tokens)\n","\n","    return combined_token\n","\n","def make_vocab(vocab, vocab_size, splits, word_freqs):\n","  while len(vocab) < vocab_size:\n","      scores = compute_pair_scores(splits)\n","      best_pair, max_score = \"\", None\n","      for pair, score in scores.items():\n","          if max_score is None or max_score < score:\n","              best_pair = pair\n","              max_score = score\n","      splits = merge_pair(*best_pair, splits, word_freqs)\n","      new_token = (\n","          best_pair[0] + best_pair[1][5:]\n","          if best_pair[1].startswith(\"<pow>\")\n","          else best_pair[0] + best_pair[1]\n","      )\n","      vocab.append(new_token)\n","  return vocab\n","\n","def encode_word(word, vocab):\n","    tokens = []\n","    while len(word) > 0:\n","        i = len(word)\n","        while i > 0 and word[:i] not in vocab:\n","            i -= 1\n","        if i == 0:\n","            return [\"<unk>\"]\n","        tokens.append(word[:i])\n","        print(tokens)\n","        word = word[i:]\n","        if len(word) > 0:\n","            word = f\"<pow>{word}\"\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fySrrflbxH0v"},"outputs":[],"source":["word_freqs = get_word_freqs(corpus)\n","alphabet = get_alphabet(word_freqs)\n","vocab = get_vocab(alphabet)\n","splits = get_split_words(word_freqs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1724648628000,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"pWzxQgI_KQL6","outputId":"be1ebc87-5e08-4653-87da-b8215418a9b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["('T', '<pow>h'): 0.125\n","('<pow>h', '<pow>i'): 0.03409090909090909\n","('<pow>i', '<pow>s'): 0.02727272727272727\n","('i', '<pow>s'): 0.1\n","('t', '<pow>h'): 0.03571428571428571\n","('<pow>h', '<pow>e'): 0.011904761904761904\n"]}],"source":["pair_scores = compute_pair_scores(splits)\n","for i, key in enumerate(pair_scores.keys()):\n","    print(f\"{key}: {pair_scores[key]}\")\n","    if i >= 5:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1724648628001,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"pFDcZ-JtKLny","outputId":"1484e336-4b5b-43db-d79d-5346b99f3adf"},"outputs":[{"name":"stdout","output_type":"stream","text":["('a', '<pow>b') 0.2\n"]}],"source":["best_pair = \"\"\n","max_score = None\n","for pair, score in pair_scores.items():\n","    if max_score is None or max_score < score:\n","        best_pair = pair\n","        max_score = score\n","\n","vocab.append(combine_tokens(best_pair))\n","print(best_pair, max_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1724648628001,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"4EvFXffVKY4n","outputId":"cdbbaac6-276d-498f-ada2-5a74522239ad"},"outputs":[{"data":{"text/plain":["['ab', '<pow>o', '<pow>u', '<pow>t']"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["splits = merge_pair(*best_pair, splits, word_freqs)\n","splits[\"about\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1724648628222,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"lzHYPzalLKCA","outputId":"806d3d5b-2cfb-4a14-8817-8ff15a46541d"},"outputs":[{"data":{"text/plain":["{'This': ['T', '<pow>h', '<pow>i', '<pow>s'],\n"," 'is': ['i', '<pow>s'],\n"," 'the': ['t', '<pow>h', '<pow>e'],\n"," 'Hugging': ['H', '<pow>u', '<pow>g', '<pow>g', '<pow>i', '<pow>n', '<pow>g'],\n"," 'Face': ['F', '<pow>a', '<pow>c', '<pow>e'],\n"," 'Course': ['C', '<pow>o', '<pow>u', '<pow>r', '<pow>s', '<pow>e'],\n"," '.': ['.'],\n"," 'chapter': ['c', '<pow>h', '<pow>a', '<pow>p', '<pow>t', '<pow>e', '<pow>r'],\n"," 'about': ['ab', '<pow>o', '<pow>u', '<pow>t'],\n"," 'tokenization': ['t',\n","  '<pow>o',\n","  '<pow>k',\n","  '<pow>e',\n","  '<pow>n',\n","  '<pow>i',\n","  '<pow>z',\n","  '<pow>a',\n","  '<pow>t',\n","  '<pow>i',\n","  '<pow>o',\n","  '<pow>n'],\n"," 'section': ['s', '<pow>e', '<pow>c', '<pow>t', '<pow>i', '<pow>o', '<pow>n'],\n"," 'shows': ['s', '<pow>h', '<pow>o', '<pow>w', '<pow>s'],\n"," 'several': ['s', '<pow>e', '<pow>v', '<pow>e', '<pow>r', '<pow>a', '<pow>l'],\n"," 'tokenizer': ['t',\n","  '<pow>o',\n","  '<pow>k',\n","  '<pow>e',\n","  '<pow>n',\n","  '<pow>i',\n","  '<pow>z',\n","  '<pow>e',\n","  '<pow>r'],\n"," 'algorithms': ['a',\n","  '<pow>l',\n","  '<pow>g',\n","  '<pow>o',\n","  '<pow>r',\n","  '<pow>i',\n","  '<pow>t',\n","  '<pow>h',\n","  '<pow>m',\n","  '<pow>s'],\n"," 'Hopefully': ['H',\n","  '<pow>o',\n","  '<pow>p',\n","  '<pow>e',\n","  '<pow>f',\n","  '<pow>u',\n","  '<pow>l',\n","  '<pow>l',\n","  '<pow>y'],\n"," ',': [','],\n"," 'you': ['y', '<pow>o', '<pow>u'],\n"," 'will': ['w', '<pow>i', '<pow>l', '<pow>l'],\n"," 'be': ['b', '<pow>e'],\n"," 'able': ['ab', '<pow>l', '<pow>e'],\n"," 'to': ['t', '<pow>o'],\n"," 'understand': ['u',\n","  '<pow>n',\n","  '<pow>d',\n","  '<pow>e',\n","  '<pow>r',\n","  '<pow>s',\n","  '<pow>t',\n","  '<pow>a',\n","  '<pow>n',\n","  '<pow>d'],\n"," 'how': ['h', '<pow>o', '<pow>w'],\n"," 'they': ['t', '<pow>h', '<pow>e', '<pow>y'],\n"," 'are': ['a', '<pow>r', '<pow>e'],\n"," 'trained': ['t', '<pow>r', '<pow>a', '<pow>i', '<pow>n', '<pow>e', '<pow>d'],\n"," 'and': ['a', '<pow>n', '<pow>d'],\n"," 'generate': ['g',\n","  '<pow>e',\n","  '<pow>n',\n","  '<pow>e',\n","  '<pow>r',\n","  '<pow>a',\n","  '<pow>t',\n","  '<pow>e'],\n"," 'tokens': ['t', '<pow>o', '<pow>k', '<pow>e', '<pow>n', '<pow>s']}"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["splits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QblWkpV76vBX"},"outputs":[],"source":["vocab = make_vocab(vocab, 161, splits, word_freqs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1724648628223,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"snwrIIrQHV3j","outputId":"8aeceedc-e434-4f31-cba5-2e301df39938"},"outputs":[{"data":{"text/plain":["['<pad>',\n"," '<unk>',\n"," '<cls>',\n"," '<sep>',\n"," '<mask>',\n"," '<pow>',\n"," ',',\n"," '.',\n"," '<pow>a',\n"," '<pow>b',\n"," '<pow>c',\n"," '<pow>d',\n"," '<pow>e',\n"," '<pow>f',\n"," '<pow>g',\n"," '<pow>h',\n"," '<pow>i',\n"," '<pow>k',\n"," '<pow>l',\n"," '<pow>m',\n"," '<pow>n',\n"," '<pow>o',\n"," '<pow>p',\n"," '<pow>r',\n"," '<pow>s',\n"," '<pow>t',\n"," '<pow>u',\n"," '<pow>v',\n"," '<pow>w',\n"," '<pow>y',\n"," '<pow>z',\n"," 'C',\n"," 'F',\n"," 'H',\n"," 'T',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 's',\n"," 't',\n"," 'u',\n"," 'w',\n"," 'y',\n"," 'ab',\n"," '<pow>fu',\n"," 'Fa',\n"," 'Fac',\n"," '<pow>ct',\n"," '<pow>ful',\n"," '<pow>full',\n"," '<pow>fully',\n"," 'Th',\n"," 'ch',\n"," '<pow>hm',\n"," 'cha',\n"," 'chap',\n"," 'chapt',\n"," '<pow>thm',\n"," 'Hu',\n"," 'Hug',\n"," 'Hugg',\n"," 'sh',\n"," 'th',\n"," 'is',\n"," '<pow>thms',\n"," '<pow>za',\n"," '<pow>zat',\n"," '<pow>ut',\n"," '<pow>ta',\n"," '<pow>at',\n"," '<pow>sta',\n"," '<pow>ra',\n"," '<pow>rsta',\n"," '<pow>rat',\n"," '<pow>ur',\n"," '<pow>urs',\n"," '<pow>ws',\n"," '<pow>ral',\n"," 'tra',\n"," '<pow>lg',\n"," 'alg',\n"," 'abl',\n"," '<pow>ll',\n"," 'ar',\n"," 'Thi',\n"," 'This',\n"," 'Huggi',\n"," '<pow>izat',\n"," '<pow>izati',\n"," '<pow>cti',\n"," '<pow>iz',\n"," '<pow>ithms',\n"," 'wi',\n"," 'will',\n"," 'trai',\n"," '<pow>rithms',\n"," 'Huggin',\n"," 'Hugging',\n"," '<pow>nizati',\n"," '<pow>niz',\n"," 'un',\n"," 'und',\n"," '<pow>rstan',\n"," '<pow>rstand',\n"," 'train',\n"," 'an',\n"," 'and',\n"," '<pow>ns',\n"," 'Co',\n"," 'Cours',\n"," 'abo',\n"," 'about',\n"," 'to',\n"," 'tok',\n"," '<pow>nizatio',\n"," '<pow>nization',\n"," '<pow>ctio',\n"," '<pow>ction',\n"," 'sho',\n"," 'shows',\n"," 'algo',\n"," 'algorithms',\n"," 'Ho',\n"," 'Hop',\n"," 'yo',\n"," 'you',\n"," 'ho',\n"," 'how',\n"," 'the',\n"," 'they',\n"," 'Face',\n"," 'Course',\n"," 'chapte',\n"," 'chapter',\n"," 'toke',\n"," 'tokenization',\n"," 'tokeniz',\n"," 'tokens',\n"," 'se',\n"," 'section',\n"," 'sev',\n"," 'seve',\n"," 'several',\n"," 'tokenize',\n"," 'tokenizer',\n"," 'Hope',\n"," 'Hopefully',\n"," 'be',\n"," 'able',\n"," 'unde',\n"," 'understand',\n"," 'are',\n"," 'traine',\n"," 'trained',\n"," 'ge',\n"," 'gen',\n"," 'gene',\n"," 'generat']"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0QR6v_ovB414","outputId":"7574be4a-86e9-482c-db76-970bf520e9e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Hugging', '<pow>f', '<pow>a', '<pow>c', '<pow>e']\n"]}],"source":["print(encode_word(\"Huggingface\", vocab))\n","print(encode_word(\"HOg\", vocab))"]},{"cell_type":"markdown","metadata":{"id":"-XA-kFHtoe48"},"source":["# WordPiece"]},{"cell_type":"code","execution_count":116,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1725093927178,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"cxsLPFm_FlGe"},"outputs":[],"source":["from pickle import FALSE\n","from collections import defaultdict\n","from typing import List, Dict\n","import re\n","from tqdm import tqdm\n","\n","class WordPieceTokenizer:\n","\n","    def __init__(self, part_of_word_token:str='<pow>', unk_token:str='<unk>', spe_tokens:List[str]=['', '<pad>', '<cls>', '<sep>', '<mask>']):\n","        self.word_freqs: defaultdict\n","        self.alphabet: List[str]\n","        self.vocab: List[str]\n","        self.splits: Dict[str, List[str]]\n","        self.vocab: List[str]\n","        self.part_of_word_token = part_of_word_token\n","        self.unk_token = unk_token\n","        self.spe_tokens: List[str] = spe_tokens + [unk_token, part_of_word_token]\n","\n","    def get_vocab(self):\n","      return self.vocab\n","\n","    def load_vocab(self, vocab):\n","      self.vocab = vocab\n","\n","    def get_metadata(self):\n","        return {\n","            \"vocab_size\": self.vocab_size,\n","            \"part_of_word_token\": self.part_of_word_token,\n","            \"unk_token\": self.unk_token,\n","            \"spe_tokens\": self.spe_tokens\n","        }\n","\n","    def load_metadata(self, metadata:dict):\n","        self.vocab_size = metadata.get(\"vocab_size\", self.vocab_size)\n","        self.part_of_word_token = metadata.get(\"part_of_word_token\", self.part_of_word_token)\n","        self.unk_token = metadata.get(\"unk_token\", self.unk_token)\n","        self.spe_tokens = set(metadata.get(\"spe_tokens\", self.spe_tokens))\n","\n","    def adapt(self, corpus: List[str], vocab_size:int):\n","        self.vocab_size: int = vocab_size\n","\n","        self.word_freqs = self._get_word_freqs(corpus)\n","        self.alphabet = self._get_alphabet()\n","        self.vocab = self._set_vocab()\n","        self.splits = self._get_split_words()\n","        self.vocab = self._make_vocab()\n","\n","    def pretokenize(self, text: List[str], unwant_chars:str = \"­Ö¢«/\\ê$ìà#Ù†ë™ä·ã³'±]_›>´çö¹¬®½:ÃØÈ¶[&ôËù¾€ò»Ò„üß0²ªè|°@{îºæ}^Çï5õ¨+âøÀ\", to_lower=False):\n","        tokens = [re.findall(r'\\w+|[^\\w\\s]', self.clean_text(sentence, unwant_chars), re.UNICODE) for sentence in text]\n","        return tokens\n","\n","    def clean_text(self, text:str, unwant_chars:str=\"­Ö¢«/\\ê$ìà#Ù†ë™ä·ã³'±]_›>´çö¹¬®½:ÃØÈ¶[&ôËù¾€ò»Ò„üß0²ªè|°@{îºæ}^Çï5õ¨+âøÀ\", to_lower=False):\n","        if to_lower: text = text.lower()\n","\n","        regex = f\"[{re.escape(unwant_chars)}]\"\n","        text = re.sub(regex, \"\", text)\n","        text = re.sub(r\"[\\n\\t\\r]+\", ' ', text)\n","        text = re.sub(r\"\\s+\", ' ', text)\n","        text = re.sub(r\"\\.{3,}\", \"...\", text)\n","        text = re.sub(r\"\\.\\.+\", \".\", text)\n","\n","        text = text.strip()\n","        return text\n","\n","    def encode_word(self, word):\n","        tokens = []\n","        while len(word) > 0:\n","            i = len(word)\n","            while i > 0 and word[:i] not in self.vocab:\n","                i -= 1\n","            if i == 0:\n","                return [self.unk_token]\n","            tokens.append(word[:i])\n","            word = word[i:]\n","            if len(word) > 0:\n","                word = f'{self.part_of_word_token}{word}'\n","        return tokens\n","\n","    def tokenize(self, text: List[str]):\n","        tokens = []\n","        text = self.pretokenize(text)\n","        for sentence in text:\n","            sentence_tokens = [token for word in sentence for token in self.encode_word(word)]\n","            tokens.append(sentence_tokens)\n","        return tokens\n","\n","    def encode(self, text: List[str], max_tokens: int = None):\n","        tokens = self.tokenize(text)\n","\n","        if not hasattr(self, 'vocab_to_index'):\n","            self.vocab_to_index = {token: idx for idx, token in enumerate(self.vocab)}\n","\n","        encoded_text = []\n","        for sentence in tokens:\n","            if max_tokens:\n","                encoded_sentence = [\n","                    self.vocab_to_index.get(token, self.vocab_to_index.get(self.unk_token, 0))\n","                    for token in sentence[:max_tokens]\n","                ]\n","                if len(encoded_sentence) < max_tokens:\n","                  encoded_sentence += [0] * (max_tokens - len(encoded_sentence))\n","            else:\n","                encoded_sentence = [\n","                    self.vocab_to_index.get(token, self.vocab_to_index.get(self.unk_token, 0))\n","                    for token in sentence\n","                ]\n","\n","            encoded_text.append(encoded_sentence)\n","\n","\n","\n","        return encoded_text\n","\n","    def decode(self, encoded_text: List[List[int]]):\n","        if not hasattr(self, 'index_to_vocab'):\n","            self.index_to_vocab = {idx: token for idx, token in enumerate(self.vocab)}\n","\n","        decoded_text = []\n","        for sentence in encoded_text:\n","            decoded_sentence = [self.index_to_vocab.get(index, self.unk_token) for index in sentence]\n","            joined_sentence = self._join_tokens(decoded_sentence)\n","            decoded_text.append(joined_sentence)\n","\n","        return decoded_text\n","\n","    def _join_tokens(self, tokens: List[str]):\n","        sentence = \"\"\n","        for token in tokens:\n","            if token.startswith(self.part_of_word_token) or token == self.spe_tokens[0]:\n","                sentence += token[len(self.part_of_word_token):]\n","            else:\n","                if sentence:\n","                    sentence += \" \"\n","                sentence += token\n","        return sentence\n","\n","    def _get_word_freqs(self, corpus):\n","      word_freqs = defaultdict(int)\n","      for text in self.pretokenize(corpus):\n","          for word in text:\n","              word_freqs[word] += 1\n","      return word_freqs\n","\n","    def _get_alphabet(self):\n","      alphabet = []\n","      for word in self.word_freqs.keys():\n","          if word[0] not in alphabet:\n","              alphabet.append(word[0])\n","          for letter in word[1:]:\n","              if f\"{self.part_of_word_token}{letter}\" not in alphabet:\n","                  alphabet.append(f\"{self.part_of_word_token}{letter}\")\n","      alphabet.sort()\n","      return alphabet\n","\n","    def _set_vocab(self):\n","      vocab = self.spe_tokens + self.alphabet.copy()\n","      return vocab\n","\n","    def _get_split_words(self):\n","      splits = {\n","        word: [c if i == 0 else f\"<pow>{c}\" for i, c in enumerate(word)] for word in self.word_freqs.keys()\n","      }\n","      return splits\n","\n","    def _compute_pair_scores(self):\n","      letter_freqs = defaultdict(int)\n","      pair_freqs = defaultdict(int)\n","      for word, freq in self.word_freqs.items():\n","          split = self.splits[word]\n","          if len(split) == 1:\n","              letter_freqs[split[0]] += freq\n","              continue\n","          for i in range(len(split) - 1):\n","              pair = (split[i], split[i + 1])\n","              letter_freqs[split[i]] += freq\n","              pair_freqs[pair] += freq\n","          letter_freqs[split[-1]] += freq\n","\n","      scores = {\n","          pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n","          for pair, freq in pair_freqs.items()\n","      }\n","      return scores\n","\n","    def _merge_pair(self, a, b):\n","      for word in self.word_freqs:\n","          split = self.splits[word]\n","          if len(split) == 1:\n","              continue\n","          i = 0\n","          while i < len(split) - 1:\n","              if split[i] == a and split[i + 1] == b:\n","                  merge = a + b.replace(self.part_of_word_token, \"\", 1) if b.startswith(\"<pow>\") else a + b\n","                  split = split[:i] + [merge] + split[i + 2 :]\n","              else:\n","                  i += 1\n","          self.splits[word] = split\n","      return self.splits\n","\n","    def _combine_tokens(self, token_pair):\n","      tokens = []\n","\n","      for i, token in enumerate(token_pair):\n","\n","          if token.startswith(self.part_of_word_token):\n","            tokens.append(token.replace(self.part_of_word_token, \"\", 1))\n","          else:\n","            tokens.append(token)\n","      combined_token = ''.join(tokens)\n","\n","      return combined_token\n","\n","    def _make_vocab(self):\n","\n","      with tqdm(total=self.vocab_size, desc=\"Procesing\") as pbar:\n","        vocab_len = 0\n","        while vocab_len < self.vocab_size:\n","            try:\n","                scores = self._compute_pair_scores()\n","                best_pair, max_score = \"\", None\n","                for pair, score in scores.items():\n","                    if max_score is None or max_score < score:\n","                        best_pair = pair\n","                        max_score = score\n","                self.splits = self._merge_pair(*best_pair)\n","                new_token = (\n","                    best_pair[0] + best_pair[1].replace(self.part_of_word_token, \"\", 1)\n","                    if best_pair[1].startswith(self.part_of_word_token)\n","                    else best_pair[0] + best_pair[1]\n","                )\n","                self.vocab.append(new_token)\n","\n","                vocab_len = len(self.vocab)\n","                delta = vocab_len - pbar.n\n","\n","                pbar.update(delta)\n","\n","\n","            except:\n","                break\n","\n","      return self.vocab\n","\n"]},{"cell_type":"code","execution_count":117,"metadata":{"id":"rgzONn4SFnkF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725093931140,"user_tz":300,"elapsed":249,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"42a7300c-6e78-4c64-d9d8-1ef830c6b7f9"},"outputs":[{"output_type":"stream","name":"stderr","text":["Procesing: 100%|██████████| 100/100 [00:00<00:00, 8310.16it/s]\n"]}],"source":["tokenizer = WordPieceTokenizer()\n","tokenizer.adapt(corpus, 100)"]},{"cell_type":"code","execution_count":118,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1725093932092,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"U8uDzG4mG1LG","outputId":"8ba427c0-2f4f-4464-fe47-431a38674ae8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['Huggi',\n","  '<pow>n',\n","  '<pow>g',\n","  'Fac',\n","  '<pow>e',\n","  'is',\n","  'c',\n","  '<pow>o',\n","  '<pow>o',\n","  '<pow>l',\n","  '.'],\n"," ['<unk>', '<unk>', 'is', 'c', '<pow>o', '<pow>o', '<pow>l', '.']]"]},"metadata":{},"execution_count":118}],"source":["tokenizer.tokenize([\"Hugging Face is cool.\", \"deep learning is cool.\"])"]},{"cell_type":"code","execution_count":121,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1725093942957,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"bMjWmvpMMYzD","outputId":"c7f1e6f7-43b5-442b-de24-d5fae4420e1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[90, 21, 15, 5, 67, 38, 22, 22, 19, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["encoded = tokenizer.encode([\"Hugging face is cool.\", \"deep learning is cool.\"], max_tokens=500)\n","print(encoded[0])"]},{"cell_type":"code","execution_count":122,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1725093944757,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"B6H7Det3NyQH","outputId":"3c7db8ef-9cdc-490c-e438-311b1f9d43e7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hugging <unk> is cool .', '<unk> <unk> is cool .']"]},"metadata":{},"execution_count":122}],"source":["tokenizer.decode(encoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xy0FzNZ8OlvG"},"outputs":[],"source":["tokenizer.vocab"]},{"cell_type":"markdown","metadata":{"id":"e3Ym1pByC11v"},"source":["# Tokenize Spanish Corpus"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28277,"status":"ok","timestamp":1725089985405,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"i1DTSrWdC5Pm","outputId":"b788b1f0-7b2f-4839-c07b-bdbc83be8cf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/Exposiciones/Chatbot/data/text.zip\n","replace data/text.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: data/text.txt           \n"]}],"source":["!unzip /content/drive/MyDrive/Exposiciones/Chatbot/data/text.zip -d \"data/\""]},{"cell_type":"code","execution_count":68,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725089985406,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"YaP4AP21M9qR"},"outputs":[],"source":["import os\n","\n","with open('/content/data/text.txt', 'r') as f:\n","    text = f.read()"]},{"cell_type":"code","source":["set_text = {*text}"],"metadata":{"id":"x9bay7ApNqsx","executionInfo":{"status":"ok","timestamp":1725089985689,"user_tz":300,"elapsed":286,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["tokenizer.clean_text(''.join(list(set_text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oSVROIo-N5UZ","executionInfo":{"status":"ok","timestamp":1725089988964,"user_tz":300,"elapsed":233,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"10f317f0-3cdf-4e96-e763-18481a83a096"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'éÜybRCMpHF.mfWG zuK-vÉnSÁkhAg XáZE2)cVÑ!d3×wJOT;\"qe%7í81óil6(ÓoPYUt¿sBQr¡úD ñÍxN4=9L,?ÚjIa'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1725084790661,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"},"user_tz":300},"id":"Hi550szUOAxQ"},"outputs":[],"source":["data = [text, ]"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wMGfEjb3Od6U","executionInfo":{"status":"ok","timestamp":1725084879243,"user_tz":300,"elapsed":37643,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"785eb781-7fb2-487e-b7fb-6cbbf4c215b4"},"outputs":[{"output_type":"stream","name":"stderr","text":["Procesing: 100%|██████████| 300/300 [00:35<00:00,  8.34it/s]\n"]}],"source":["tokenizer = WordPieceTokenizer(1000)\n","tokenizer.adapt(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUVAlZprTNza"},"outputs":[],"source":["_vocab = tokenizer.get_vocab()\n","\n","with open('/content/drive/MyDrive/Exposiciones/Chatbot/data/vocab.txt', 'w') as f:\n","    f.write('\\n'.join(_vocab))"]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/Exposiciones/Chatbot/data/vocab.txt', 'r') as f:\n","    _vocab = f.read().split('\\n')\n","\n","tokenizer = WordPieceTokenizer(10000)\n","tokenizer.vocab = _vocab"],"metadata":{"id":"5Uy8WMeU-KWm","executionInfo":{"status":"ok","timestamp":1725083309852,"user_tz":300,"elapsed":1450,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.tokenize([\"Me encantan los rollos de canela.\", \"saco a pasear a mi perro por las mañanas\"]))\n","encoded = tokenizer.encode([\"Me encantan los rollos de canela.\", \"saco a pasear a mi perro por las mañanas\"])\n","print(encoded)\n","tokenizer.decode(encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fz-Qc3Vq-nmq","executionInfo":{"status":"ok","timestamp":1725084158895,"user_tz":300,"elapsed":241,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"285d9b71-ccbc-4be7-b03c-5805aec9a6fc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[['M', '<pow>e', 'e', '<pow>n', '<pow>c', '<pow>a', '<pow>n', '<pow>t', '<pow>a', '<pow>n', 'l', '<pow>o', '<pow>s', 'r', '<pow>o', '<pow>l', '<pow>l', '<pow>o', '<pow>s', 'd', '<pow>e', 'c', '<pow>a', '<pow>n', '<pow>e', '<pow>l', '<pow>a', '.'], ['s', '<pow>a', '<pow>c', '<pow>o', 'a', 'p', '<pow>a', '<pow>s', '<pow>e', '<pow>a', '<pow>r', 'a', 'm', '<pow>i', 'p', '<pow>e', '<pow>r', '<pow>r', '<pow>o', 'p', '<pow>o', '<pow>r', 'l', '<pow>a', '<pow>s', 'm', '<pow>a', '<pow>ñ', '<pow>a', '<pow>n', '<pow>a', '<pow>s']]\n","[[154, 73, 177, 82, 71, 69, 82, 88, 69, 82, 184, 83, 87, 190, 83, 80, 80, 83, 87, 176, 73, 175, 69, 82, 73, 80, 69, 18], [191, 69, 71, 83, 173, 188, 69, 87, 73, 69, 86, 173, 185, 77, 188, 73, 86, 86, 83, 188, 83, 86, 184, 69, 87, 185, 69, 128, 69, 82, 69, 87]]\n"]},{"output_type":"execute_result","data":{"text/plain":["['Me encantan los rollos de canela .',\n"," 'saco a pasear a mi perro por las mañanas']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Comparation"],"metadata":{"id":"Fo60V1_Dl82G"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n"],"metadata":{"id":"P8pHN-QHl-su","executionInfo":{"status":"ok","timestamp":1725093679851,"user_tz":300,"elapsed":3841,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"execution_count":104,"outputs":[]},{"cell_type":"code","source":["# Configuración del vectorizador de texto\n","max_tokens = 10000  # Tamaño del vocabulario (número máximo de tokens únicos)\n","max_sequence_length = 50  # Longitud máxima de la secuencia\n","\n","vectorizer = TextVectorization(\n","    max_tokens=max_tokens,\n","    ragged=False,  # Usa secuencias de longitud fija\n","    output_sequence_length=max_sequence_length\n",")\n"],"metadata":{"id":"H6zE2aBRmAmH","executionInfo":{"status":"ok","timestamp":1725093679851,"user_tz":300,"elapsed":3,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","\n","# Datos de ejemplo\n","sentences = [\n","    \"Este es un ejemplo de texto.\",\n","    \"Aquí hay otro ejemplo.\",\n","    \"Vamos a tokenizar estos textos.\"\n","]\n","\n","# Crear la instancia de TextVectorization\n","max_tokens = 10000\n","max_sequence_length = 10  # Ajusta según sea necesario\n","\n","vectorizer = TextVectorization(\n","    max_tokens=max_tokens,\n","    output_sequence_length=max_sequence_length\n",")\n","\n","# Adaptar el vectorizador al texto\n","vectorizer.adapt(sentences)\n","\n","# Transformar texto en vectores\n","vectorized_texts = vectorizer(sentences)\n","\n","# Mostrar resultados\n","print(\"Vocabulario:\", vectorizer.get_vocabulary())\n","print(\"Textos vectorizados:\")\n","print(vectorized_texts.numpy())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LebmCePKmD13","executionInfo":{"status":"ok","timestamp":1725093679851,"user_tz":300,"elapsed":3,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"11c396fe-e446-42df-a84f-bb5bbc7eadc9"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulario: ['', '[UNK]', 'ejemplo', 'vamos', 'un', 'tokenizar', 'textos', 'texto', 'otro', 'hay', 'estos', 'este', 'es', 'de', 'aquí', 'a']\n","Textos vectorizados:\n","[[11 12  4  2 13  7  0  0  0  0]\n"," [14  9  8  2  0  0  0  0  0  0]\n"," [ 3 15  5 10  6  0  0  0  0  0]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ewCpekvvmFeg"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1RlisDce7tSadg2rQKLIvqw-wO-odrGMo","authorship_tag":"ABX9TyMq8ObyM8dV9MengW+HfiTR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}