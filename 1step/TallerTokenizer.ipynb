{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Ckp__npVzxjaa4eEhXg7-rQ5QDM23yvt","timestamp":1726377261697}],"authorship_tag":"ABX9TyNhBgI2Z5EFGIQBLd07XzIw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SetUp"],"metadata":{"id":"A_BZqSrU1EWW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4hqr8ZCxMTD"},"outputs":[],"source":["%%capture\n","!git clone https://github.com/Ryuksito/chatbot.git"]},{"cell_type":"code","source":["import json"],"metadata":{"id":"M2cdr2cR2PUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VOCAB_PATH = '/content/chatbot/weights/vocab.txt'\n","METADATA_PATH = '/content/chatbot/weights/metadata.json'\n","\n","with open(VOCAB_PATH, 'r') as f:\n","  vocab = f.read().split('\\n')\n","\n","with open(METADATA_PATH, 'r') as f:\n","  metadata = json.load(f)\n","\n","corpus = [\n","    \"la vida es un hermoso viaje.\",\n","    \"nunca es tarde para aprender algo nuevo.\",\n","    \"el Ã©xito es la suma de pequeÃ±os esfuerzos repetidos dÃ­a tras dÃ­a.\",\n","    \"La perseverancia es la clave para alcanzar tus sueÃ±os.\",\n","    \"La creatividad es la inteligencia divirtiÃ©ndose.\"\n","]\n","\n","texts = [\n","    \"El futuro pertenece a quienes creen en sus sueÃ±os.\",\n","    \"Los obstÃ¡culos son esas cosas que ves cuando apartas la vista de tu meta.\",\n","    \"El trabajo duro supera al talento cuando el talento no trabaja duro.\",\n","    \"Cree en ti mismo y serÃ¡s imparable.\",\n","    \"Las grandes ideas nacen de grandes desafÃ­os.\"\n","]\n","\n","def pprint(sequence, break_num=5):\n","  for i, part in enumerate(sequence):\n","    print(f'|{part}|,  ', end='')\n","    if (i+1) % break_num == 0:\n","        print('\\n')"],"metadata":{"id":"E_SgOPtM2PxS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## WordPiece Tokenizer"],"metadata":{"id":"dkmkZvI81K50"}},{"cell_type":"code","source":["from pickle import FALSE\n","from collections import defaultdict\n","from typing import List, Dict\n","import re\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","_unwanted_chars = \"\"\"Â­Ã–Â¢Â«/\\Ãª$Ã¬Ã #Ã™â€ Ã«â„¢Ã¤Â·Ã£Â³'Â±]_â€º>Â´Ã§Ã¶Â¹Â¬Â®Â½:ÃƒÃ˜ÃˆÂ¶[&Ã´Ã‹Ã¹Â¾â‚¬Ã²Â»Ã’â€žÃ¼ÃŸ0Â²ÂªÃ¨|Â°@{Ã®ÂºÃ¦}^Ã‡Ã¯5ÃµÂ¨+Ã¢Ã¸Ã€\"%*<=`~Â£Â¥Â©ÂµÂ¼Ã…Ã—Ã°Ã·Ã»Ä€ÄÄ“Ä›Ä«Ä±Å‚ÅÅ“ÅŸÅ«Å¼Ç’ÇœÈ³É™É›ÉªÊƒË†ËˆËšÌÌ„Ì©Î“Î”Î£Î©Î±Î²Î³Î´ÎµÎ¸ÎºÎ»Î¼Î½Ï€ÏÏƒÏˆà¤—à¤¯à¥‹â€‹â€â€“â€”â€–â€˜â€™â€œâ€â€¢â€¦â°â´â·âºâ»â‚€â‚â‚‚â‚„â‚†â‚¹â„ƒâ„‰â…“â…”â†â†’â†“â†”â‡Œâˆ‚âˆƒâˆ‘âˆ’âˆ™âˆšâˆžâˆ âˆ£âˆ§âˆ¨âˆ©â‰ˆâ‰ â‰¡â‰¤â‰¥â‹…â°â”€â”‚â”Œâ”â””â”˜â”œâ”¤â”¬â”´â”¼â•â•‘â•”â•—â•šâ•â• â•£â•¦â•©â•¬â–ˆâ–‘â–“â–¼â˜€â˜â˜…â˜•â˜ â™€â™‚â™•â™–â™™â™›â™œâ™Ÿâ™»âš âš½â›°âœ…âœˆâœ¨â„â¤âž•âž–âž¡â­ã€‚ã€Œã€ã‚ã„ã†ãŒãã”ã–ã™ãŸã¡ã¨ã¯ã¾ã‚Šã‚‹ã‚’ä¸€ä»åƒå›°å‚å¤•å¤¢å¤©å¤ªå¤±å­å¼·å¾—å¾³å¿…æ„šæ˜Žæ™ºæœˆæœ‰æœ¬æ¨™æº–çš‡ç›®ç§è€…è–è™‘èªžé›£é¦¬é¾ï¬ï¸ï¼Œï¼›ï¿½ðŸ†•ðŸ‡¦ðŸ‡§ðŸ‡¨ðŸ‡©ðŸ‡ªðŸ‡«ðŸ‡¬ðŸ‡­ðŸ‡®ðŸ‡¯ðŸ‡°ðŸ‡±ðŸ‡²ðŸ‡³ðŸ‡´ðŸ‡µðŸ‡¶ðŸ‡·ðŸ‡¸ðŸ‡¹ðŸ‡ºðŸ‡½ðŸ‡¿ðŸŒ€ðŸŒ…ðŸŒŠðŸŒðŸŒŽðŸŒðŸŒžðŸŒŸðŸŒ§ðŸŒ±ðŸŒ²ðŸŒ³ðŸŒ´ðŸŒ¸ðŸŒ»ðŸŒ¿ðŸƒðŸŠðŸŒðŸŽðŸ“ðŸ•ðŸžðŸ©ðŸªðŸ¯ðŸ³ðŸ´ðŸ¿ðŸŽ‚ðŸŽˆðŸŽ‰ðŸŽŠðŸŽ“ðŸŽ¥ðŸŽ§ðŸŽ¨ðŸŽ¬ðŸŽ®ðŸŽµðŸŽ¶ðŸŽ¼ðŸŽ¾ðŸ€ðŸƒðŸˆðŸŠðŸ’ðŸ–ðŸ¡ðŸ»ðŸ½ðŸ¾ðŸˆðŸ˜ðŸ›ðŸ¥ðŸ¨ðŸ®ðŸ°ðŸ±ðŸ¶ðŸ¹ðŸ¾ðŸ‘‡ðŸ‘ŠðŸ‘ŒðŸ‘ðŸ‘“ðŸ‘ ðŸ‘¨ðŸ‘©ðŸ’†ðŸ’‰ðŸ’•ðŸ’–ðŸ’™ðŸ’šðŸ’¡ðŸ’¦ðŸ’ªðŸ’«ðŸ’¯ðŸ’°ðŸ’³ðŸ’¸ðŸ’»ðŸ“ˆðŸ“–ðŸ“šðŸ“¢ðŸ“£ðŸ“²ðŸ“ºðŸ”ðŸ”’ðŸ”¥ðŸ”¹ðŸ•·ðŸ—³ðŸ—ºðŸ˜€ðŸ˜ðŸ˜‚ðŸ˜†ðŸ˜ˆðŸ˜ŠðŸ˜‹ðŸ˜ðŸ˜ŽðŸ˜”ðŸ˜±ðŸ˜´ðŸ™ŒðŸ™ðŸš€ðŸš‚ðŸš†ðŸš‡ðŸšŒðŸš—ðŸš¨ðŸšªðŸš°ðŸš´ðŸš¶ðŸ›ðŸŸ¢ðŸ¤–ðŸ¤ðŸ¤©ðŸ¥•ðŸ¥¦ðŸ¦ðŸ¦‡ðŸ¦ŠðŸ¦¸ðŸ§ðŸ§˜ðŸ§´ðŸ§¸ðŸ§¹ðŸ§¼ðŸª€\"\"\"\n","\n","\n","class WordPieceTokenizer:\n","\n","    def __init__(self, seq_length:int=None, part_of_word_token:str='<pow>', unk_token:str='<unk>', spe_tokens:List[str]=['', '<bos>', '<eos>', '<sep>', '<mask>']):\n","        self.word_freqs: defaultdict\n","        self.alphabet: List[str]\n","        self.vocab: List[str]\n","        self.splits: Dict[str, List[str]]\n","        self.vocab: List[str]\n","        self.part_of_word_token = part_of_word_token\n","        self.unk_token = unk_token\n","        self.seq_length = seq_length\n","        self.spe_tokens: List[str] = spe_tokens + [unk_token, part_of_word_token]\n","\n","\n","    #------------------------------Make Vocab-------------------------------------------\n","\n","    def get_vocab(self):\n","      return self.vocab\n","\n","    def load_vocab(self, vocab):\n","      self.vocab = vocab\n","\n","    def get_metadata(self):\n","        return {\n","            \"vocab_size\": self.vocab_size,\n","            \"part_of_word_token\": self.part_of_word_token,\n","            \"unk_token\": self.unk_token,\n","            \"spe_tokens\": self.spe_tokens\n","        }\n","\n","    def load_metadata(self, metadata:dict):\n","        self.vocab_size = metadata[\"vocab_size\"]\n","        self.part_of_word_token = metadata[\"part_of_word_token\"]\n","        self.unk_token = metadata[\"unk_token\"]\n","        self.spe_tokens = metadata[\"spe_tokens\"]\n","\n","    def adapt(self, corpus: List[str], vocab_size:int):\n","        self.vocab_size: int = vocab_size\n","\n","        self.word_freqs = self._get_word_freqs(corpus)\n","        self.alphabet = self._get_alphabet()\n","        self.vocab = self._set_vocab()\n","        self.splits = self._get_split_words()\n","        self.vocab = self._make_vocab()\n","\n","    #------------------------------Encode Text-------------------------------------------\n","\n","    def clean_text(self, text_tensor, unwant_chars:str=_unwanted_chars, to_lower=False):\n","        if to_lower:\n","            text_tensor = tf.strings.lower(text_tensor)\n","\n","        unwant_chars_pattern = \"[\" + re.escape(unwant_chars) + \"]\"\n","        text_tensor = tf.strings.regex_replace(text_tensor, unwant_chars_pattern, \"\")\n","        text_tensor = tf.strings.regex_replace(text_tensor, r\"[\\n\\t\\r]+\", \" \")\n","        text_tensor = tf.strings.regex_replace(text_tensor, r\"\\s+\", \" \")\n","        text_tensor = tf.strings.regex_replace(text_tensor, r\"\\.{3,}\", \"...\")\n","        text_tensor = tf.strings.regex_replace(text_tensor, r\"\\.\\.+\", \".\")\n","\n","        text_tensor = tf.strings.strip(text_tensor)\n","\n","        return text_tensor\n","\n","    def pretokenize(self, text_tensor, unwant_chars=_unwanted_chars): # tested function\n","        cleaned_text = self.clean_text(text_tensor, unwant_chars)\n","\n","        decoded_text = tf.strings.unicode_decode(cleaned_text, 'UTF-8')\n","\n","        encoded_text = tf.strings.unicode_encode(decoded_text, 'UTF-8')\n","\n","        tokens = tf.strings.regex_replace(\n","            encoded_text,\n","            r'(\\p{L}+|\\p{N}+|\\p{P})',\n","            r' \\1 '\n","        )\n","\n","        tokens = tf.strings.split(tokens)\n","\n","        return tokens\n","\n","    def encode_word(self, word): # tested function\n","        tokens = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n","\n","        def condition(word, tokens):\n","            return tf.strings.length(word) > 0\n","\n","        def body(word, tokens):\n","            length = tf.strings.length(word)\n","            i = length\n","\n","            def sub_condition(i, word):\n","                substr = tf.strings.substr(word, 0, i)\n","                is_in_vocab = tf.reduce_any(tf.equal(self.vocab, substr))\n","                return tf.logical_and(tf.greater(i, 0), tf.logical_not(is_in_vocab))\n","            def decrement_i(i, word):\n","                return i - tf.convert_to_tensor(1), word\n","            i, word = tf.while_loop(sub_condition, decrement_i, [i, word])\n","\n","            if i == tf.constant(0):\n","\n","                token = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n","                token = tokens.write(0, tf.convert_to_tensor(self.unk_token))\n","                return tf.convert_to_tensor(''), token\n","\n","\n","\n","            tokens = tokens.write(tokens.size(), tf.strings.substr(word, 0, i))\n","            word = tf.strings.substr(word, i, -1)\n","\n","\n","            if tf.strings.length(word) > 0:\n","                word = tf.strings.join([self.part_of_word_token, word])\n","\n","\n","            return word, tokens\n","\n","        word, tokens = tf.while_loop(condition, body, [word, tokens])\n","\n","        t = tokens.stack()\n","        return t\n","\n","    def tokenize(self, sentence, null_token:bool=False): # tested function\n","        tokens = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n","\n","        words = self.pretokenize(sentence)\n","\n","        word_index = tf.constant(0)\n","\n","        def sentence_condition(word_index, tokens, words):\n","            return tf.not_equal(word_index, tf.size(words))\n","        def sentence_body(word_index, tokens, words):\n","            word = words[word_index]\n","\n","            def word_condition(word, tokens):\n","                return tf.strings.length(word) > 0\n","            def word_body(word, tokens):\n","                length = tf.strings.length(word)\n","                i = length\n","\n","                def sub_condition(i, word):\n","                    substr = tf.strings.substr(word, 0, i)\n","                    is_in_vocab = tf.reduce_any(tf.equal(self.vocab, substr))\n","                    return tf.logical_and(tf.greater(i, 0), tf.logical_not(is_in_vocab))\n","                def decrement_i(i, word):\n","                    return i - tf.convert_to_tensor(1), word\n","                i, word = tf.while_loop(sub_condition, decrement_i, [i, word])\n","\n","                if i == tf.constant(0):\n","\n","                    token = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n","                    token = tokens.write(0, tf.convert_to_tensor(self.unk_token))\n","                    return tf.convert_to_tensor(''), token\n","\n","\n","\n","                tokens = tokens.write(tokens.size(), tf.strings.substr(word, 0, i))\n","                word = tf.strings.substr(word, i, -1)\n","\n","\n","                if tf.strings.length(word) > 0:\n","                    word = tf.strings.join([self.part_of_word_token, word])\n","\n","\n","                return word, tokens\n","\n","            word, tokens = tf.while_loop(word_condition, word_body, [word, tokens])\n","\n","            return word_index + 1, tokens, words\n","\n","        word_index, tokens, words = tf.while_loop(sentence_condition, sentence_body, [word_index, tokens, words])\n","\n","        tokenized_text = tokens.stack()\n","        current_num_tokens = tf.size(tokenized_text)\n","\n","\n","        def truncate_tokens():\n","            truncated_tokens = tf.slice(tokenized_text, [0], [self.seq_length -1])\n","            return truncated_tokens\n","\n","        def pad_tokens():\n","            pad_length = (self.seq_length - 1) - current_num_tokens\n","            padding = tf.fill([pad_length], self.vocab[0])\n","            return tf.concat([tokenized_text, padding], axis=0)\n","\n","        def process_tokens():\n","            return tf.cond(current_num_tokens > self.seq_length,\n","                          true_fn=truncate_tokens,\n","                          false_fn=pad_tokens)\n","\n","        tokenized_text = tf.cond(\n","            tf.convert_to_tensor(null_token),\n","            true_fn=process_tokens,\n","            false_fn=lambda: tokenized_text\n","        )\n","\n","        return tokenized_text\n","\n","    def format_sequence(self, tokenized_text, begin_token = True, end_token=True):\n","        if begin_token:\n","          tokenized_text = tf.concat([[self.spe_tokens[1],], tokenized_text], axis=0)\n","\n","        current_num_tokens = tf.size(tokenized_text)\n","\n","        def truncate_tokens():\n","            truncated_tokens = tf.slice(tokenized_text, [0], [self.seq_length -1])\n","            if (end_token):\n","              return tf.concat([truncated_tokens, [self.spe_tokens[2],]], axis=0)\n","            else:\n","              return truncated_tokens\n","\n","        def pad_tokens():\n","            pad_length = (self.seq_length - 1) - current_num_tokens\n","            padding = tf.fill([pad_length], self.vocab[0])\n","            if end_token:\n","              concated_tokens = tf.concat([tokenized_text, [self.spe_tokens[2], ], padding], axis=0)\n","            else:\n","              concated_tokens = tf.concat([tokenized_text, padding], axis=0)\n","            return concated_tokens\n","\n","        return tf.cond(current_num_tokens > self.seq_length,\n","                          true_fn=truncate_tokens,\n","                          false_fn=pad_tokens)\n","\n","    def encode(self, sentence, answer=None, begin_token = True, sep_token=False, end_token=True):\n","        tokenized_sentence = self.tokenize(sentence, null_token=False)\n","        if answer is not None:\n","          tokens_list = [\n","              tokenized_sentence,\n","              [self.spe_tokens[3], ],\n","              self.tokenize(answer, null_token=False)\n","          ]\n","        else:\n","          tokens_list = [\n","              tokenized_sentence,\n","              [self.spe_tokens[3], ],\n","          ]\n","\n","        tokens = tf.concat(\n","            tokens_list,\n","            axis=0\n","        )\n","        tokens = self.format_sequence(tokens, begin_token=begin_token, end_token=end_token)\n","\n","        if not hasattr(self, 'vocab_to_index'):\n","            self.vocab_to_index = {token: idx for idx, token in enumerate(self.vocab)}\n","\n","        vocab_tensor = tf.constant(list(self.vocab_to_index.keys()), dtype=tf.string)\n","        index_tensor = tf.constant(list(self.vocab_to_index.values()), dtype=tf.int32)\n","\n","        def lookup_token(token):\n","            token_index = tf.where(tf.equal(vocab_tensor, token), index_tensor, -1)\n","            token_index = tf.reduce_max(token_index)\n","            return tf.cond(token_index == -1,\n","                           lambda: self.vocab_to_index[self.unk_token],\n","                           lambda: token_index)\n","\n","        token_indices = tf.map_fn(lookup_token, tokens, fn_output_signature=tf.int32)\n","\n","        return token_indices\n","\n","    #------------------------------Decode Text-------------------------------------------\n","    def decode(self, encoded_text):\n","        if not hasattr(self, 'index_to_vocab'):\n","            self.index_to_vocab = {idx: token for idx, token in enumerate(self.vocab)}\n","\n","        index_tensor = tf.constant(list(self.index_to_vocab.keys()), dtype=tf.int32)\n","        vocab_tensor = tf.constant(list(self.index_to_vocab.values()), dtype=tf.string)\n","\n","        def lookup_index(index):\n","            token = vocab_tensor[index]\n","\n","            def is_part_of_word():\n","                word = tf.strings.substr(token, len(self.part_of_word_token), -1)\n","                return word\n","            def not_part_of_word():\n","                return ' ' + token\n","\n","            token = tf.cond(\n","                tf.equal(tf.strings.substr(token, 0, len(self.part_of_word_token)), self.part_of_word_token),\n","                is_part_of_word,\n","                not_part_of_word\n","            )\n","            return token\n","\n","        decoded_text_tensor = tf.map_fn(lookup_index, encoded_text, dtype=tf.string)\n","\n","        decoded_text = decoded_text_tensor\n","\n","        decoded_text = tf.strings.reduce_join(decoded_text_tensor, separator='')\n","\n","        return decoded_text.numpy().decode('utf-8').strip()\n","\n","\n","    #------------------------------Make vocab functions-------------------------------------------\n","\n","    def _join_tokens(self, tokens: List[str]):\n","        sentence = \"\"\n","        for token in tokens:\n","            if token.startswith(self.part_of_word_token) or token == self.spe_tokens[0]:\n","                sentence += token[len(self.part_of_word_token):]\n","            else:\n","                if sentence:\n","                    sentence += \" \"\n","                sentence += token\n","        return sentence\n","\n","    def _get_word_freqs(self, corpus):\n","      word_freqs = defaultdict(int)\n","      for text in self.pretokenize(corpus):\n","          for word in text:\n","              word = word.numpy().decode('utf-8')\n","              word_freqs[word] += 1\n","      return word_freqs\n","\n","    def _get_alphabet(self):\n","      alphabet = []\n","      for word in self.word_freqs.keys():\n","          if word[0] not in alphabet:\n","              alphabet.append(word[0])\n","          for letter in word[1:]:\n","              if f\"{self.part_of_word_token}{letter}\" not in alphabet:\n","                  alphabet.append(f\"{self.part_of_word_token}{letter}\")\n","      alphabet.sort()\n","      return alphabet\n","\n","    def _set_vocab(self):\n","      vocab = self.spe_tokens + self.alphabet.copy()\n","      return vocab\n","\n","    def _get_split_words(self):\n","      splits = {\n","        word: [c if i == 0 else f\"<pow>{c}\" for i, c in enumerate(word)] for word in self.word_freqs.keys()\n","      }\n","      return splits\n","\n","    def _compute_pair_scores(self):\n","      letter_freqs = defaultdict(int)\n","      pair_freqs = defaultdict(int)\n","      for word, freq in self.word_freqs.items():\n","          split = self.splits[word]\n","          if len(split) == 1:\n","              letter_freqs[split[0]] += freq\n","              continue\n","          for i in range(len(split) - 1):\n","              pair = (split[i], split[i + 1])\n","              letter_freqs[split[i]] += freq\n","              pair_freqs[pair] += freq\n","          letter_freqs[split[-1]] += freq\n","\n","      scores = {\n","          pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n","          for pair, freq in pair_freqs.items()\n","      }\n","      return scores\n","\n","    def _merge_pair(self, a, b):\n","      for word in self.word_freqs:\n","          split = self.splits[word]\n","          if len(split) == 1:\n","              continue\n","          i = 0\n","          while i < len(split) - 1:\n","              if split[i] == a and split[i + 1] == b:\n","                  merge = a + b.replace(self.part_of_word_token, \"\", 1) if b.startswith(\"<pow>\") else a + b\n","                  split = split[:i] + [merge] + split[i + 2 :]\n","              else:\n","                  i += 1\n","          self.splits[word] = split\n","      return self.splits\n","\n","    def _combine_tokens(self, token_pair):\n","      tokens = []\n","\n","      for i, token in enumerate(token_pair):\n","\n","          if token.startswith(self.part_of_word_token):\n","            tokens.append(token.replace(self.part_of_word_token, \"\", 1))\n","          else:\n","            tokens.append(token)\n","      combined_token = ''.join(tokens)\n","\n","      return combined_token\n","\n","    def _make_vocab(self):\n","\n","      with tqdm(total=self.vocab_size, desc=\"Procesing\") as pbar:\n","        vocab_len = 0\n","        while vocab_len < self.vocab_size:\n","            try:\n","                scores = self._compute_pair_scores()\n","                best_pair, max_score = \"\", None\n","                for pair, score in scores.items():\n","                    if max_score is None or max_score < score:\n","                        best_pair = pair\n","                        max_score = score\n","                self.splits = self._merge_pair(*best_pair)\n","                new_token = (\n","                    best_pair[0] + best_pair[1].replace(self.part_of_word_token, \"\", 1)\n","                    if best_pair[1].startswith(self.part_of_word_token)\n","                    else best_pair[0] + best_pair[1]\n","                )\n","                self.vocab.append(new_token)\n","\n","                vocab_len = len(self.vocab)\n","                delta = vocab_len - pbar.n\n","\n","                pbar.update(delta)\n","\n","\n","            except:\n","                break\n","\n","      return self.vocab\n"],"metadata":{"id":"PJt5-Tu81NzK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Adaptar el tokenizer al texto de prueba\n","\n","\n","*   Probar primero con un vocab_size de 100\n","*   Probar despues con un vocab_size de 500\n","\n"],"metadata":{"id":"Gvpd-Ixg2-lk"}},{"cell_type":"code","source":[],"metadata":{"id":"NFhP1E5NG11t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualizar el vocabulario adaptado"],"metadata":{"id":"AR6xL0F57zSd"}},{"cell_type":"code","source":[],"metadata":{"id":"6NWF37W56u-e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cargar pesos del tokenizer"],"metadata":{"id":"jGUGkog_15Vg"}},{"cell_type":"code","source":[],"metadata":{"id":"RiD9jf7c91lE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualizar el vocabulario adaptado"],"metadata":{"id":"EVzhZuMa-R0L"}},{"cell_type":"code","source":[],"metadata":{"id":"pCovQ72cG759"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pretokenizar el texto\n","\n","\n"],"metadata":{"id":"4Vqjl2l01nrV"}},{"cell_type":"code","source":[],"metadata":{"id":"eEE4Ede5G8y9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizar el texto\n","Importante a tomar en cuenta\n","\n","\n","*   Tokenizer Adaptado: esta adaptado con poco texto de prueba solo 100 tokens de vocabulario\n","*   Tokenizer Cargado: esta cargado con 15000 tokens de vocabulario unicamente funciona con palabras minusculas"],"metadata":{"id":"VUXe2nsJ2I9z"}},{"cell_type":"markdown","source":["## Texto Conocido"],"metadata":{"id":"H-gmdhnvCXSE"}},{"cell_type":"code","source":[],"metadata":{"id":"IzFPBzdwG-B-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Texto Desconocido"],"metadata":{"id":"5BqfYMGfCaBR"}},{"cell_type":"code","source":[],"metadata":{"id":"rmAqmJ6sHERW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Codificar el texto\n","\n","Importante\n","*   Al momento de crear el objeto tokenizer es importante definir el ***seq_length***=***N***, en un entero mayor a 0, esto para usar la funcion encode.\n","*   De lo contrario para evitar el error de usar el metodo encode sin ***seq_length*** definido se debe usar el metodo ***encode_word***\n","\n"],"metadata":{"id":"0K8ydsxY2Lel"}},{"cell_type":"markdown","source":["## Texto Conocido"],"metadata":{"id":"3NW2Ip2CCeF5"}},{"cell_type":"code","source":[],"metadata":{"id":"5c2eERxAHF-1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Texto Desconocido"],"metadata":{"id":"8VMB8My-Cgy6"}},{"cell_type":"code","source":[],"metadata":{"id":"DwQNU14QHHYN"},"execution_count":null,"outputs":[]}]}