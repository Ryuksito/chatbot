{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPtkaO8QfbZxl0jQqi9Wa4T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SetUp"],"metadata":{"id":"A_BZqSrU1EWW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4hqr8ZCxMTD"},"outputs":[],"source":["%%capture\n","!git clone https://github.com/Ryuksito/chatbot.git"]},{"cell_type":"code","source":["import json"],"metadata":{"id":"M2cdr2cR2PUK","executionInfo":{"status":"ok","timestamp":1726375021159,"user_tz":300,"elapsed":216,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["VOCAB_PATH = '/content/chatbot/weights/vocab.txt'\n","METADATA_PATH = '/content/chatbot/weights/metadata.json'\n","\n","with open(VOCAB_PATH, 'r') as f:\n","  vocab = f.read().split('\\n')\n","\n","with open(METADATA_PATH, 'r') as f:\n","  metadata = json.load(f)\n","\n","corpus = [\n","    \"la vida es un hermoso viaje.\",\n","    \"nunca es tarde para aprender algo nuevo.\",\n","    \"el éxito es la suma de pequeños esfuerzos repetidos día tras día.\",\n","    \"La perseverancia es la clave para alcanzar tus sueños.\",\n","    \"La creatividad es la inteligencia divirtiéndose.\"\n","]\n","\n","texts = [\n","    \"El futuro pertenece a quienes creen en sus sueños.\",\n","    \"Los obstáculos son esas cosas que ves cuando apartas la vista de tu meta.\",\n","    \"El trabajo duro supera al talento cuando el talento no trabaja duro.\",\n","    \"Cree en ti mismo y serás imparable.\",\n","    \"Las grandes ideas nacen de grandes desafíos.\"\n","]\n","\n","def pprint(sequence, break_num=5):\n","  for i, part in enumerate(sequence):\n","    print(f'|{part}|,  ', end='')\n","    if (i+1) % break_num == 0:\n","        print('\\n')"],"metadata":{"id":"E_SgOPtM2PxS","executionInfo":{"status":"ok","timestamp":1726376459703,"user_tz":300,"elapsed":202,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":["## WordPiece Tokenizer"],"metadata":{"id":"dkmkZvI81K50"}},{"cell_type":"code","source":["from pickle import FALSE\n","from collections import defaultdict\n","from typing import List, Dict\n","import re\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","_unwanted_chars = \"\"\"­Ö¢«/\\ê$ìà#Ù†ë™ä·ã³'±]_›>´çö¹¬®½:ÃØÈ¶[&ôËù¾€ò»Ò„üß0²ªè|°@{îºæ}^Çï5õ¨+âøÀ\"%*<=`~£¥©µ¼Å×ð÷ûĀāēěīıłōœşūżǒǜȳəɛɪʃˆˈ˚̩́̄ΓΔΣΩαβγδεθκλμνπρσψगयो​‍–—‖‘’“”•…⁰⁴⁷⁺⁻₀₁₂₄₆₹℃℉⅓⅔←→↓↔⇌∂∃∑−∙√∞∠∣∧∨∩≈≠≡≤≥⋅⏰─│┌┐└┘├┤┬┴┼═║╔╗╚╝╠╣╦╩╬█░▓▼☀☁★☕☠♀♂♕♖♙♛♜♟♻⚠⚽⛰✅✈✨❄❤➕➖➡⭐。「」あいうがくござすたちとはまりるを一仁千困坂夕夢天太失子強得徳必愚明智月有本標準皇目私者聖虑語難馬龍ﬁ️，；�🆕🇦🇧🇨🇩🇪🇫🇬🇭🇮🇯🇰🇱🇲🇳🇴🇵🇶🇷🇸🇹🇺🇽🇿🌀🌅🌊🌍🌎🌏🌞🌟🌧🌱🌲🌳🌴🌸🌻🌿🍃🍊🍌🍎🍓🍕🍞🍩🍪🍯🍳🍴🍿🎂🎈🎉🎊🎓🎥🎧🎨🎬🎮🎵🎶🎼🎾🏀🏃🏈🏊🏒🏖🏡🏻🏽🏾🐈🐘🐛🐥🐨🐮🐰🐱🐶🐹🐾👇👊👌👏👓👠👨👩💆💉💕💖💙💚💡💦💪💫💯💰💳💸💻📈📖📚📢📣📲📺🔍🔒🔥🔹🕷🗳🗺😀😁😂😆😈😊😋😍😎😔😱😴🙌🙏🚀🚂🚆🚇🚌🚗🚨🚪🚰🚴🚶🛍🟢🤖🤝🤩🥕🥦🦁🦇🦊🦸🧁🧘🧴🧸🧹🧼🪀\"\"\"\n","\n","\n","class WordPieceTokenizer:\n","\n","    def __init__(self, seq_length:int=None, part_of_word_token:str='<pow>', unk_token:str='<unk>', spe_tokens:List[str]=['', '<bos>', '<eos>', '<sep>', '<mask>']):\n","        self.word_freqs: defaultdict\n","        self.alphabet: List[str]\n","        self.vocab: List[str]\n","        self.splits: Dict[str, List[str]]\n","        self.vocab: List[str]\n","        self.part_of_word_token = part_of_word_token\n","        self.unk_token = unk_token\n","        self.seq_length = seq_length\n","        self.spe_tokens: List[str] = spe_tokens + [unk_token, part_of_word_token]\n","\n","\n","    #------------------------------Make Vocab-------------------------------------------\n","\n","    def get_vocab(self):\n","      return self.vocab\n","\n","    def load_vocab(self, vocab):\n","      self.vocab = vocab\n","\n","    def get_metadata(self):\n","        return {\n","            \"vocab_size\": self.vocab_size,\n","            \"part_of_word_token\": self.part_of_word_token,\n","            \"unk_token\": self.unk_token,\n","            \"spe_tokens\": self.spe_tokens\n","        }\n","\n","    def load_metadata(self, metadata:dict):\n","        self.vocab_size = metadata[\"vocab_size\"]\n","        self.part_of_word_token = metadata[\"part_of_word_token\"]\n","        self.unk_token = metadata[\"unk_token\"]\n","        self.spe_tokens = metadata[\"spe_tokens\"]\n","\n","    def adapt(self, corpus: List[str], vocab_size:int):\n","        self.vocab_size: int = vocab_size\n","\n","        self.word_freqs = self._get_word_freqs(corpus)\n","        self.alphabet = self._get_alphabet()\n","        self.vocab = self._set_vocab()\n","        self.splits = self._get_split_words()\n","        self.vocab = self._make_vocab()\n","\n","    #------------------------------Encode Text-------------------------------------------\n","\n","    def clean_text(self, text_tensor, unwant_chars:str=_unwanted_chars, to_lower=False):\n","        if to_lower:\n","            text_tensor = tf.strings.lower(text_tensor)\n","\n","        unwant_chars_pattern = \"[\" + re.escape(unwant_chars) + \"]\"\n","        text_tensor = tf.strings.regex_replace(text_tensor, unwant_chars_pattern, \"\")\n","        text_tensor = tf.strings.regex_replace(text_tensor, r\"[\\n\\t\\r]+\", \" \")\n","        text_tensor = tf.strings.regex_replace(text_tensor, r\"\\s+\", \" \")\n","        text_tensor = tf.strings.regex_replace(text_tensor, r\"\\.{3,}\", \"...\")\n","        text_tensor = tf.strings.regex_replace(text_tensor, r\"\\.\\.+\", \".\")\n","\n","        text_tensor = tf.strings.strip(text_tensor)\n","\n","        return text_tensor\n","\n","    def pretokenize(self, text_tensor, unwant_chars=_unwanted_chars): # tested function\n","        cleaned_text = self.clean_text(text_tensor, unwant_chars)\n","\n","        decoded_text = tf.strings.unicode_decode(cleaned_text, 'UTF-8')\n","\n","        encoded_text = tf.strings.unicode_encode(decoded_text, 'UTF-8')\n","\n","        tokens = tf.strings.regex_replace(\n","            encoded_text,\n","            r'(\\p{L}+|\\p{N}+|\\p{P})',\n","            r' \\1 '\n","        )\n","\n","        tokens = tf.strings.split(tokens)\n","\n","        return tokens\n","\n","    def encode_word(self, word): # tested function\n","        tokens = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n","\n","        def condition(word, tokens):\n","            return tf.strings.length(word) > 0\n","\n","        def body(word, tokens):\n","            length = tf.strings.length(word)\n","            i = length\n","\n","            def sub_condition(i, word):\n","                substr = tf.strings.substr(word, 0, i)\n","                is_in_vocab = tf.reduce_any(tf.equal(self.vocab, substr))\n","                return tf.logical_and(tf.greater(i, 0), tf.logical_not(is_in_vocab))\n","            def decrement_i(i, word):\n","                return i - tf.convert_to_tensor(1), word\n","            i, word = tf.while_loop(sub_condition, decrement_i, [i, word])\n","\n","            if i == tf.constant(0):\n","\n","                token = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n","                token = tokens.write(0, tf.convert_to_tensor(self.unk_token))\n","                return tf.convert_to_tensor(''), token\n","\n","\n","\n","            tokens = tokens.write(tokens.size(), tf.strings.substr(word, 0, i))\n","            word = tf.strings.substr(word, i, -1)\n","\n","\n","            if tf.strings.length(word) > 0:\n","                word = tf.strings.join([self.part_of_word_token, word])\n","\n","\n","            return word, tokens\n","\n","        word, tokens = tf.while_loop(condition, body, [word, tokens])\n","\n","        t = tokens.stack()\n","        return t\n","\n","    def tokenize(self, sentence, null_token:bool=False): # tested function\n","        tokens = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n","\n","        words = self.pretokenize(sentence)\n","\n","        word_index = tf.constant(0)\n","\n","        def sentence_condition(word_index, tokens, words):\n","            return tf.not_equal(word_index, tf.size(words))\n","        def sentence_body(word_index, tokens, words):\n","            word = words[word_index]\n","\n","            def word_condition(word, tokens):\n","                return tf.strings.length(word) > 0\n","            def word_body(word, tokens):\n","                length = tf.strings.length(word)\n","                i = length\n","\n","                def sub_condition(i, word):\n","                    substr = tf.strings.substr(word, 0, i)\n","                    is_in_vocab = tf.reduce_any(tf.equal(self.vocab, substr))\n","                    return tf.logical_and(tf.greater(i, 0), tf.logical_not(is_in_vocab))\n","                def decrement_i(i, word):\n","                    return i - tf.convert_to_tensor(1), word\n","                i, word = tf.while_loop(sub_condition, decrement_i, [i, word])\n","\n","                if i == tf.constant(0):\n","\n","                    token = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n","                    token = tokens.write(0, tf.convert_to_tensor(self.unk_token))\n","                    return tf.convert_to_tensor(''), token\n","\n","\n","\n","                tokens = tokens.write(tokens.size(), tf.strings.substr(word, 0, i))\n","                word = tf.strings.substr(word, i, -1)\n","\n","\n","                if tf.strings.length(word) > 0:\n","                    word = tf.strings.join([self.part_of_word_token, word])\n","\n","\n","                return word, tokens\n","\n","            word, tokens = tf.while_loop(word_condition, word_body, [word, tokens])\n","\n","            return word_index + 1, tokens, words\n","\n","        word_index, tokens, words = tf.while_loop(sentence_condition, sentence_body, [word_index, tokens, words])\n","\n","        tokenized_text = tokens.stack()\n","        current_num_tokens = tf.size(tokenized_text)\n","\n","\n","        def truncate_tokens():\n","            truncated_tokens = tf.slice(tokenized_text, [0], [self.seq_length -1])\n","            return truncated_tokens\n","\n","        def pad_tokens():\n","            pad_length = (self.seq_length - 1) - current_num_tokens\n","            padding = tf.fill([pad_length], self.vocab[0])\n","            return tf.concat([tokenized_text, padding], axis=0)\n","\n","        def process_tokens():\n","            return tf.cond(current_num_tokens > self.seq_length,\n","                          true_fn=truncate_tokens,\n","                          false_fn=pad_tokens)\n","\n","        tokenized_text = tf.cond(\n","            tf.convert_to_tensor(null_token),\n","            true_fn=process_tokens,\n","            false_fn=lambda: tokenized_text\n","        )\n","\n","        return tokenized_text\n","\n","    def format_sequence(self, tokenized_text, begin_token = True, end_token=True):\n","        if begin_token:\n","          tokenized_text = tf.concat([[self.spe_tokens[1],], tokenized_text], axis=0)\n","\n","        current_num_tokens = tf.size(tokenized_text)\n","\n","        def truncate_tokens():\n","            truncated_tokens = tf.slice(tokenized_text, [0], [self.seq_length -1])\n","            if (end_token):\n","              return tf.concat([truncated_tokens, [self.spe_tokens[2],]], axis=0)\n","            else:\n","              return truncated_tokens\n","\n","        def pad_tokens():\n","            pad_length = (self.seq_length - 1) - current_num_tokens\n","            padding = tf.fill([pad_length], self.vocab[0])\n","            if end_token:\n","              concated_tokens = tf.concat([tokenized_text, [self.spe_tokens[2], ], padding], axis=0)\n","            else:\n","              concated_tokens = tf.concat([tokenized_text, padding], axis=0)\n","            return concated_tokens\n","\n","        return tf.cond(current_num_tokens > self.seq_length,\n","                          true_fn=truncate_tokens,\n","                          false_fn=pad_tokens)\n","\n","    def encode(self, sentence, answer=None, begin_token = True, sep_token=False, end_token=True):\n","        tokenized_sentence = self.tokenize(sentence, null_token=False)\n","        if answer is not None:\n","          tokens_list = [\n","              tokenized_sentence,\n","              [self.spe_tokens[3], ],\n","              self.tokenize(answer, null_token=False)\n","          ]\n","        else:\n","          tokens_list = [\n","              tokenized_sentence,\n","              [self.spe_tokens[3], ],\n","          ]\n","\n","        tokens = tf.concat(\n","            tokens_list,\n","            axis=0\n","        )\n","        tokens = self.format_sequence(tokens, begin_token=begin_token, end_token=end_token)\n","\n","        if not hasattr(self, 'vocab_to_index'):\n","            self.vocab_to_index = {token: idx for idx, token in enumerate(self.vocab)}\n","\n","        vocab_tensor = tf.constant(list(self.vocab_to_index.keys()), dtype=tf.string)\n","        index_tensor = tf.constant(list(self.vocab_to_index.values()), dtype=tf.int32)\n","\n","        def lookup_token(token):\n","            token_index = tf.where(tf.equal(vocab_tensor, token), index_tensor, -1)\n","            token_index = tf.reduce_max(token_index)\n","            return tf.cond(token_index == -1,\n","                           lambda: self.vocab_to_index[self.unk_token],\n","                           lambda: token_index)\n","\n","        token_indices = tf.map_fn(lookup_token, tokens, fn_output_signature=tf.int32)\n","\n","        return token_indices\n","\n","    #------------------------------Decode Text-------------------------------------------\n","    def decode(self, encoded_text):\n","        if not hasattr(self, 'index_to_vocab'):\n","            self.index_to_vocab = {idx: token for idx, token in enumerate(self.vocab)}\n","\n","        index_tensor = tf.constant(list(self.index_to_vocab.keys()), dtype=tf.int32)\n","        vocab_tensor = tf.constant(list(self.index_to_vocab.values()), dtype=tf.string)\n","\n","        def lookup_index(index):\n","            token = vocab_tensor[index]\n","\n","            def is_part_of_word():\n","                word = tf.strings.substr(token, len(self.part_of_word_token), -1)\n","                return word\n","            def not_part_of_word():\n","                return ' ' + token\n","\n","            token = tf.cond(\n","                tf.equal(tf.strings.substr(token, 0, len(self.part_of_word_token)), self.part_of_word_token),\n","                is_part_of_word,\n","                not_part_of_word\n","            )\n","            return token\n","\n","        decoded_text_tensor = tf.map_fn(lookup_index, encoded_text, dtype=tf.string)\n","\n","        decoded_text = decoded_text_tensor\n","\n","        decoded_text = tf.strings.reduce_join(decoded_text_tensor, separator='')\n","\n","        return decoded_text.numpy().decode('utf-8').strip()\n","\n","\n","    #------------------------------Make vocab functions-------------------------------------------\n","\n","    def _join_tokens(self, tokens: List[str]):\n","        sentence = \"\"\n","        for token in tokens:\n","            if token.startswith(self.part_of_word_token) or token == self.spe_tokens[0]:\n","                sentence += token[len(self.part_of_word_token):]\n","            else:\n","                if sentence:\n","                    sentence += \" \"\n","                sentence += token\n","        return sentence\n","\n","    def _get_word_freqs(self, corpus):\n","      word_freqs = defaultdict(int)\n","      for text in self.pretokenize(corpus):\n","          for word in text:\n","              word = word.numpy().decode('utf-8')\n","              word_freqs[word] += 1\n","      return word_freqs\n","\n","    def _get_alphabet(self):\n","      alphabet = []\n","      for word in self.word_freqs.keys():\n","          if word[0] not in alphabet:\n","              alphabet.append(word[0])\n","          for letter in word[1:]:\n","              if f\"{self.part_of_word_token}{letter}\" not in alphabet:\n","                  alphabet.append(f\"{self.part_of_word_token}{letter}\")\n","      alphabet.sort()\n","      return alphabet\n","\n","    def _set_vocab(self):\n","      vocab = self.spe_tokens + self.alphabet.copy()\n","      return vocab\n","\n","    def _get_split_words(self):\n","      splits = {\n","        word: [c if i == 0 else f\"<pow>{c}\" for i, c in enumerate(word)] for word in self.word_freqs.keys()\n","      }\n","      return splits\n","\n","    def _compute_pair_scores(self):\n","      letter_freqs = defaultdict(int)\n","      pair_freqs = defaultdict(int)\n","      for word, freq in self.word_freqs.items():\n","          split = self.splits[word]\n","          if len(split) == 1:\n","              letter_freqs[split[0]] += freq\n","              continue\n","          for i in range(len(split) - 1):\n","              pair = (split[i], split[i + 1])\n","              letter_freqs[split[i]] += freq\n","              pair_freqs[pair] += freq\n","          letter_freqs[split[-1]] += freq\n","\n","      scores = {\n","          pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n","          for pair, freq in pair_freqs.items()\n","      }\n","      return scores\n","\n","    def _merge_pair(self, a, b):\n","      for word in self.word_freqs:\n","          split = self.splits[word]\n","          if len(split) == 1:\n","              continue\n","          i = 0\n","          while i < len(split) - 1:\n","              if split[i] == a and split[i + 1] == b:\n","                  merge = a + b.replace(self.part_of_word_token, \"\", 1) if b.startswith(\"<pow>\") else a + b\n","                  split = split[:i] + [merge] + split[i + 2 :]\n","              else:\n","                  i += 1\n","          self.splits[word] = split\n","      return self.splits\n","\n","    def _combine_tokens(self, token_pair):\n","      tokens = []\n","\n","      for i, token in enumerate(token_pair):\n","\n","          if token.startswith(self.part_of_word_token):\n","            tokens.append(token.replace(self.part_of_word_token, \"\", 1))\n","          else:\n","            tokens.append(token)\n","      combined_token = ''.join(tokens)\n","\n","      return combined_token\n","\n","    def _make_vocab(self):\n","\n","      with tqdm(total=self.vocab_size, desc=\"Procesing\") as pbar:\n","        vocab_len = 0\n","        while vocab_len < self.vocab_size:\n","            try:\n","                scores = self._compute_pair_scores()\n","                best_pair, max_score = \"\", None\n","                for pair, score in scores.items():\n","                    if max_score is None or max_score < score:\n","                        best_pair = pair\n","                        max_score = score\n","                self.splits = self._merge_pair(*best_pair)\n","                new_token = (\n","                    best_pair[0] + best_pair[1].replace(self.part_of_word_token, \"\", 1)\n","                    if best_pair[1].startswith(self.part_of_word_token)\n","                    else best_pair[0] + best_pair[1]\n","                )\n","                self.vocab.append(new_token)\n","\n","                vocab_len = len(self.vocab)\n","                delta = vocab_len - pbar.n\n","\n","                pbar.update(delta)\n","\n","\n","            except:\n","                break\n","\n","      return self.vocab\n"],"metadata":{"id":"PJt5-Tu81NzK","executionInfo":{"status":"ok","timestamp":1726374078488,"user_tz":300,"elapsed":185,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## Adaptar el tokenizer al texto de prueba\n","\n","\n","*   Probar primero con un vocab_size de 100\n","*   Probar despues con un vocab_size de 500\n","\n"],"metadata":{"id":"Gvpd-Ixg2-lk"}},{"cell_type":"code","source":["# Crear un objeto tokenizer\n","tokenizer = WordPieceTokenizer(seq_length=10)\n","\n","# Adaptar el tokenizer al texto de prueba y generar un maximo de 100 tokens\n","tokenizer.adapt(corpus, vocab_size=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-EAe9pvP3Cqq","executionInfo":{"status":"ok","timestamp":1726377133582,"user_tz":300,"elapsed":202,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"603dc3e9-1430-479b-bb80-884789b6943c"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stderr","text":["Procesing: 100%|██████████| 100/100 [00:00<00:00, 4321.35it/s]\n"]}]},{"cell_type":"markdown","source":["### Visualizar el vocabulario adaptado"],"metadata":{"id":"AR6xL0F57zSd"}},{"cell_type":"code","source":["for i, token in enumerate(tokenizer.vocab[0:50]):\n","    print(f'|{token}|,  ', end='')\n","    if (i+1) % 10 == 0:\n","        print('\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NWF37W56u-e","executionInfo":{"status":"ok","timestamp":1726375109747,"user_tz":300,"elapsed":3,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"ed5c7419-58e8-4c47-a7d4-935aac01853a"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["||,  |<bos>|,  |<eos>|,  |<sep>|,  |<mask>|,  |<unk>|,  |<pow>|,  |.|,  |<pow>a|,  |<pow>c|,  \n","\n","|<pow>d|,  |<pow>e|,  |<pow>f|,  |<pow>g|,  |<pow>i|,  |<pow>j|,  |<pow>l|,  |<pow>m|,  |<pow>n|,  |<pow>o|,  \n","\n","|<pow>p|,  |<pow>q|,  |<pow>r|,  |<pow>s|,  |<pow>t|,  |<pow>u|,  |<pow>v|,  |<pow>x|,  |<pow>z|,  |<pow>é|,  \n","\n","|<pow>í|,  |<pow>ñ|,  |L|,  |a|,  |c|,  |d|,  |e|,  |h|,  |i|,  |l|,  \n","\n","|n|,  |p|,  |r|,  |s|,  |t|,  |u|,  |v|,  |é|,  |éx|,  |dí|,  \n","\n"]}]},{"cell_type":"markdown","source":["## Cargar pesos del tokenizer"],"metadata":{"id":"jGUGkog_15Vg"}},{"cell_type":"code","source":["tokenizer = WordPieceTokenizer(seq_length=500)\n","tokenizer.load_vocab(vocab)\n","tokenizer.load_metadata(metadata)"],"metadata":{"id":"RiD9jf7c91lE","executionInfo":{"status":"ok","timestamp":1726375055331,"user_tz":300,"elapsed":193,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["### Visualizar el vocabulario adaptado"],"metadata":{"id":"EVzhZuMa-R0L"}},{"cell_type":"code","source":["for i, token in enumerate(tokenizer.vocab[0:50]):\n","    print(f'|{token}|,  ', end='')\n","    if (i+1) % 10 == 0:\n","        print('\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lh_TG2aD-SSe","executionInfo":{"status":"ok","timestamp":1726375088642,"user_tz":300,"elapsed":184,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"68ada0b2-7613-45b0-9436-9ed14e5ec222"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["||,  |<bos>|,  |<eos>|,  |<sep>|,  |<mask>|,  |<unk>|,  |<pow>|,  |!|,  |(|,  |)|,  \n","\n","|,|,  |-|,  |.|,  |1|,  |2|,  |3|,  |4|,  |6|,  |7|,  |8|,  \n","\n","|9|,  |;|,  |<pow>1|,  |<pow>2|,  |<pow>3|,  |<pow>4|,  |<pow>6|,  |<pow>7|,  |<pow>8|,  |<pow>9|,  \n","\n","|<pow>a|,  |<pow>b|,  |<pow>c|,  |<pow>d|,  |<pow>e|,  |<pow>f|,  |<pow>g|,  |<pow>h|,  |<pow>i|,  |<pow>j|,  \n","\n","|<pow>k|,  |<pow>l|,  |<pow>m|,  |<pow>n|,  |<pow>o|,  |<pow>p|,  |<pow>q|,  |<pow>r|,  |<pow>s|,  |<pow>t|,  \n","\n"]}]},{"cell_type":"markdown","source":["# Pretokenizar el texto\n","\n","\n"],"metadata":{"id":"4Vqjl2l01nrV"}},{"cell_type":"code","source":["text = corpus[0]\n","\n","print('Texto Conocido: ', text, '\\n')\n","\n","pprint(tokenizer.pretokenize(text), 4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BG8tpLOK-cVE","executionInfo":{"status":"ok","timestamp":1726376532640,"user_tz":300,"elapsed":201,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"76b555c7-c56d-45cb-f279-21b1b5376bda"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Texto Conocido:  la vida es un hermoso viaje. \n","\n","|b'la'|,  |b'vida'|,  |b'es'|,  |b'un'|,  \n","\n","|b'hermoso'|,  |b'viaje'|,  |b'.'|,  "]}]},{"cell_type":"markdown","source":["# Tokenizar el texto\n","Importante a tomar en cuenta\n","\n","\n","*   Tokenizer Adaptado: esta adaptado con poco texto de prueba solo 100 tokens de vocabulario\n","*   Tokenizer Cargado: esta cargado con 15000 tokens de vocabulario unicamente funciona con palabras minusculas"],"metadata":{"id":"VUXe2nsJ2I9z"}},{"cell_type":"markdown","source":["## Texto Conocido"],"metadata":{"id":"H-gmdhnvCXSE"}},{"cell_type":"code","source":["text1 = corpus[0]\n","\n","print('Texto Conocido: ', text1, '\\n')\n","\n","tokenized = tokenizer.tokenize(text1)\n","\n","pprint(tokenized, 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XosLIwrJClVR","executionInfo":{"status":"ok","timestamp":1726376579383,"user_tz":300,"elapsed":180,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"4fdcfabb-6750-4f9b-bbf2-936b6abe4068"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["Texto Conocido:  la vida es un hermoso viaje. \n","\n","|b'la'|,  |b'vida'|,  |b'es'|,  |b'un'|,  |b'hermoso'|,  \n","\n","|b'viaje'|,  |b'.'|,  "]}]},{"cell_type":"markdown","source":["## Texto Desconocido"],"metadata":{"id":"5BqfYMGfCaBR"}},{"cell_type":"code","source":["text2 = texts[0]\n","print('Texto Desconocido: ', text2, '\\n')\n","\n","tokenized = tokenizer.tokenize(text2)\n","\n","pprint(tokenized, 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hoZC8ZhsESKr","executionInfo":{"status":"ok","timestamp":1726376682231,"user_tz":300,"elapsed":440,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"ee82bcaf-a6b8-4af5-a111-420f5b780f13"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Texto Desconocido:  El futuro pertenece a quienes creen en sus sueños. \n","\n","|b'<unk>'|,  |b'p'|,  |b'<pow>e'|,  |b'<pow>r'|,  |b'<pow>t'|,  \n","\n","|b'<pow>e'|,  |b'<pow>n'|,  |b'<pow>e'|,  |b'<pow>c'|,  |b'<pow>e'|,  \n","\n","|b'a'|,  |b'cre'|,  |b'<pow>e'|,  |b'<pow>n'|,  |b'e'|,  \n","\n","|b'<pow>n'|,  |b'su'|,  |b'<pow>s'|,  |b'sue\\xc3\\xb1os'|,  |b'.'|,  \n","\n"]}]},{"cell_type":"markdown","source":["# Codificar el texto\n","\n","Importante\n","*   Al momento de crear el objeto tokenizer es importante definir el ***seq_length***=***N***, en un entero mayor a 0, esto para usar la funcion encode.\n","*   De lo contrario para evitar el error de usar el metodo encode sin ***seq_length*** definido se debe usar el metodo ***encode_word***\n","\n"],"metadata":{"id":"0K8ydsxY2Lel"}},{"cell_type":"markdown","source":["## Texto Conocido"],"metadata":{"id":"3NW2Ip2CCeF5"}},{"cell_type":"code","source":["text1 = corpus[0]\n","\n","print('Texto Conocido: ', text1, '\\n')\n","\n","tokenized = tokenizer.encode(text1)\n","\n","pprint(tokenized, 5)\n","\n","print('Texto decodificado: ', tokenizer.decode(tokenized))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zf4g9rpKEweB","executionInfo":{"status":"ok","timestamp":1726377190214,"user_tz":300,"elapsed":647,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"593b0898-9b3e-4399-d2d4-d1f370b595e2"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Texto Conocido:  la vida es un hermoso viaje. \n","\n","|1|,  |39|,  |8|,  |76|,  |8|,  \n","\n","|36|,  |23|,  |61|,  |37|,  |2|,  \n","\n","Texto decodificado:  <bos> la vida es un h <eos>\n"]}]},{"cell_type":"markdown","source":["## Texto Desconocido"],"metadata":{"id":"8VMB8My-Cgy6"}},{"cell_type":"code","source":["text1 = texts[0]\n","\n","print('Texto Conocido: ', text1, '\\n')\n","\n","tokenized = tokenizer.encode(text1)\n","\n","pprint(tokenized, 5)\n","\n","print('Texto decodificado: ', tokenizer.decode(tokenized))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AuVdA9UlExWG","executionInfo":{"status":"ok","timestamp":1726377245511,"user_tz":300,"elapsed":583,"user":{"displayName":"Alan_twt","userId":"10060741533168240015"}},"outputId":"5327a9ba-9dcd-4358-ff28-a8ca4bfdbf35"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Texto Conocido:  El futuro pertenece a quienes creen en sus sueños. \n","\n","|1|,  |5|,  |41|,  |11|,  |22|,  \n","\n","|24|,  |11|,  |18|,  |11|,  |2|,  \n","\n","Texto decodificado:  <bos> <unk> pertene <eos>\n"]}]}]}